{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91a18b6",
   "metadata": {},
   "source": [
    "\n",
    "Q1. Difference between Simple Linear Regression and Multiple Linear Regression:\n",
    "\n",
    "Simple Linear Regression: In simple linear regression, there is only one independent variable (predictor variable) used to predict the dependent variable. The relationship between the independent and dependent variables is assumed to be linear. The equation for simple linear regression is of the form: \n",
    "\n",
    "y=mx+b, where \n",
    "\n",
    "y is the dependent variable, \n",
    "\n",
    "x is the independent variable, \n",
    "\n",
    "m is the slope, and \n",
    "\n",
    "b is the intercept.\n",
    "\n",
    "Example: Predicting house prices based on the area of the house.\n",
    "\n",
    "Multiple Linear Regression: In multiple linear regression, there are multiple independent variables used to predict the dependent variable. The relationship between the independent variables and the dependent variable is assumed to be linear. The equation for multiple linear regression is of the form: \n",
    "\n",
    "y=b \n",
    "0\n",
    "​\n",
    " +b \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +b \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+b \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " , where \n",
    "\n",
    "y is the dependent variable, \n",
    "\n",
    "​\n",
    "  are the independent variables, and \n",
    "\n",
    "​\n",
    "  are the coefficients.\n",
    "\n",
    "Example: Predicting house prices based on area, number of bedrooms, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e3f9d",
   "metadata": {},
   "source": [
    "Q2. Assumptions of Linear Regression:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables should be linear.\n",
    "Independence: The residuals (errors) should be independent of each other.\n",
    "Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\n",
    "Normality: The residuals should be normally distributed.\n",
    "No multicollinearity: The independent variables should not be highly correlated with each other.\n",
    "Checking Assumptions:\n",
    "\n",
    "Linearity: Use scatter plots or residual plots.\n",
    "Independence: Use autocorrelation plots.\n",
    "Homoscedasticity: Use residual plots or Breusch-Pagan test.\n",
    "Normality: Use Q-Q plots or Shapiro-Wilk test.\n",
    "No multicollinearity: Use correlation matrices or Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea221bf",
   "metadata": {},
   "source": [
    "<!-- Q3. Interpretation of Slope and Intercept:\n",
    "\n",
    "Slope (\n",
    "\n",
    "m): It represents the change in the dependent variable (\n",
    "\n",
    "y) for a one-unit change in the independent variable (\n",
    "\n",
    "x).\n",
    "Example: In a simple linear regression model predicting exam scores (\n",
    "\n",
    "y) based on hours of study (\n",
    "\n",
    "x), a slope of 0.5 means that for every additional hour of study, the exam score increases by 0.5 points.\n",
    "\n",
    "Intercept (\n",
    "\n",
    "b): It represents the value of the dependent variable (\n",
    "\n",
    "y) when all independent variables are equal to zero.\n",
    "Example: In the same regression model, if the intercept is 40, it means that if a student doesn't study at all (\n",
    "\n",
    "=\n",
    "0\n",
    "x=0), the predicted exam score would be 40. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cff113ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Interpretation of Slope and Intercept:\n",
    "\n",
    "# Slope (\n",
    "\n",
    "# m): It represents the change in the dependent variable (\n",
    "\n",
    "# y) for a one-unit change in the independent variable (\n",
    "\n",
    "# x).\n",
    "# Example: In a simple linear regression model predicting exam scores (\n",
    "\n",
    "# y) based on hours of study (\n",
    "\n",
    "# x), a slope of 0.5 me# Q4. Gradient Descent:\n",
    "\n",
    "# Gradient descent is an optimization algorithm used to minimize the cost function (or loss function) of a machine learning model. It iteratively adjusts the model parameters (coefficients) to reach the minimum of the cost function.\n",
    "# In each iteration, the algorithm calculates the gradient of the cost function with respect to each parameter and updates the parameters in the opposite direction of the gradient.\n",
    "# This process continues until the algorithm converges to the minimum or a predefined number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed86e8",
   "metadata": {},
   "source": [
    "Q4. Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function (or loss function) of a machine learning model. It iteratively adjusts the model parameters (coefficients) to reach the minimum of the cost function.\n",
    "In each iteration, the algorithm calculates the gradient of the cost function with respect to each parameter and updates the parameters in the opposite direction of the gradient.\n",
    "This process continues until the algorithm converges to the minimum or a predefined number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3de552",
   "metadata": {},
   "source": [
    "Q5. Multiple Linear Regression Model:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression where there are multiple independent variables used to predict the dependent variable.\n",
    "It differs from simple linear regression in that it considers the effect of multiple predictors on the dependent variable simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c115c",
   "metadata": {},
   "source": [
    "Q6. Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated with each other.\n",
    "It can cause issues such as unstable coefficient estimates and difficulty in interpreting the effects of individual predictors.\n",
    "Multicollinearity can be detected using correlation matrices or Variance Inflation Factor (VIF) analysis.\n",
    "To address multicollinearity, potential solutions include removing one of the correlated variables, using dimensionality reduction techniques, or collecting more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46068dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
